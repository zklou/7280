{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67147f3",
   "metadata": {},
   "source": [
    "# Assignment 4: Modeling Epidemics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef5e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import EoN as eon\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "import EoN as eon\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce2f937",
   "metadata": {},
   "source": [
    "## Part 1: Outbreak Modeling [40 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d0e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flu_network():\n",
    "    # Read the graph\n",
    "    G = nx.read_edgelist(\"fludata.txt\", nodetype=int, data=((\"weight\", float),))\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7bf3fd",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da59a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_outbreak(G, n_iter, initial_infected=325, tmax=10, beta=0.01, mu=0.5):\n",
    "    \"\"\"\n",
    "    Simulates an SIS epidemic outbreak multiple times, ensuring valid epidemic spread.\n",
    "    \n",
    "    Inputs:\n",
    "        G : nx.Graph\n",
    "            The contact network graph.\n",
    "        n_iter : int\n",
    "            Number of simulation runs.\n",
    "        initial_infected : int\n",
    "            The node that is initially infected.\n",
    "        tmax : int\n",
    "            Maximum simulation time.\n",
    "        beta : float\n",
    "            Transmission rate of the disease.\n",
    "        mu : float\n",
    "            Recovery rate of the disease.\n",
    "    \n",
    "    Returns:\n",
    "        simulation_runs : list[tuple]\n",
    "            A list of tuples (t, S, I) representing the time periods, number of susceptible, \n",
    "            and number of infected individuals in each simulation.\n",
    "    \"\"\"\n",
    "    simulation_runs = []\n",
    "    successful_runs = 0\n",
    "\n",
    "    while successful_runs < n_iter:\n",
    "        t, S, I = eon.fast_SIS(G, tau=beta, gamma=mu, tmax=tmax, initial_infecteds=[initial_infected])\n",
    "\n",
    "        if I[-1] > 0:  # Âè™‰øùÁïôÊú™ÁÅ≠ÁªùÁöÑÊ®°Êãü\n",
    "            simulation_runs.append((t, S, I))\n",
    "            successful_runs += 1\n",
    "\n",
    "    return simulation_runs\n",
    "\n",
    "\n",
    "def plot_outbreaks(simulation_runs, save=False):\n",
    "    \"\"\"\n",
    "    Plots multiple epidemic outbreak simulations from SIS model.\n",
    "    \n",
    "    Inputs:\n",
    "        simulation_runs : list[tuple]\n",
    "            The list-like object of tuples returned by `simulate_outbreak`.\n",
    "        save : bool\n",
    "            Whether to save the figure.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for t, S, I in simulation_runs:\n",
    "        plt.plot(t, I, label='Infected', alpha=0.6, linestyle='-', color='r')\n",
    "        plt.plot(t, S, label='Susceptible', alpha=0.6, linestyle='--', color='b')\n",
    "    \n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Number of Individuals\")\n",
    "    plt.title(\"SIS Epidemic Outbreak Simulation\")\n",
    "    plt.legend([\"Infected\", \"Susceptible\"], loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(\"1_1.png\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb9c21",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea55f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exponent(simulation_run, I_thrsh=100):\n",
    "    \"\"\"\n",
    "    Estimates the exponential growth parameter tau using a log-transformed linear fit.\n",
    "\n",
    "    Inputs:\n",
    "        simulation_run : tuple\n",
    "            A tuple of (t, S, I) as specified in `simulate_outbreak()`\n",
    "        I_thrsh : int\n",
    "            Threshold of I. We only fit the curve where I <= I_thrsh\n",
    "\n",
    "    Returns:\n",
    "        tau : float\n",
    "    \"\"\"\n",
    "    t, _, I = simulation_run\n",
    "    mask = (np.array(I) <= I_thrsh) & (np.array(I) > 0)  # Á°Æ‰øù I>0\n",
    "    t_fit, I_fit = np.array(t)[mask], np.array(I)[mask]\n",
    "\n",
    "    if len(t_fit) < 5:\n",
    "        return 0.0  # Ëá≥Â∞ë 5 ‰∏™ÁÇπÔºåÂê¶ÂàôË∑≥ËøáÊãüÂêà\n",
    "    # Â§ÑÁêÜ log() ÈóÆÈ¢òÔºåÈÅøÂÖç log(0) Âíå NaN\n",
    "    I_fit = np.nan_to_num(I_fit, nan=1e-10, posinf=1e-10, neginf=1e-10)\n",
    "    I_fit[I_fit <= 0] = 1e-10  # Á°Æ‰øùÊâÄÊúâ I > 0ÔºåÈÅøÂÖç log(0) ÈóÆÈ¢ò\n",
    "\n",
    "    try:\n",
    "        log_I_fit = np.log(I_fit)  # Áõ¥Êé• log Â§ÑÁêÜ\n",
    "        popt, _ = scipy.optimize.curve_fit(lambda t, tau: np.exp(t / tau), t_fit, I_fit, p0=[1])\n",
    "        return popt[0]  # ËøîÂõû œÑ\n",
    "    except (RuntimeError, ValueError):\n",
    "        return 0.0  # ÊãüÂêàÂ§±Ë¥•\n",
    "\n",
    "\n",
    "\n",
    "def plot_curve_fit(simulation_run, tau, I_thrsh=100, save=False):\n",
    "    \"\"\"\n",
    "    Plots actual infected vs fitted exponential curve.\n",
    "    \n",
    "    Inputs:\n",
    "        simulation_run : tuple\n",
    "        tau : float\n",
    "        I_thrsh : int\n",
    "        save : bool\n",
    "    \n",
    "    Returns:\n",
    "        r2 : float\n",
    "            R-squared value for the fit.\n",
    "    \"\"\"\n",
    "    t, _, I = simulation_run\n",
    "    mask = (np.array(I) <= I_thrsh) & (np.array(I) > 0)  # ËøáÊª§Êéâ I <= 0 ÁöÑÁÇπ\n",
    "    t_fit, I_fit = np.array(t)[mask], np.array(I)[mask]\n",
    "\n",
    "    if len(t_fit) < 5:  # Ëá≥Â∞ë 5 ‰∏™Êï∞ÊçÆÁÇπ\n",
    "        print(\"‚ö†Ô∏è Warning: Not enough data points for fitting.\")\n",
    "        return 0.0\n",
    "\n",
    "    # Â§ÑÁêÜ log() ÈóÆÈ¢òÔºåÈÅøÂÖç log(0) Âíå NaN\n",
    "    I_fit = np.nan_to_num(I_fit, nan=1e-10, posinf=1e-10, neginf=1e-10)\n",
    "    I_fit[I_fit <= 0] = 1e-10  # Á°Æ‰øùÊâÄÊúâ I > 0ÔºåÈÅøÂÖç log(0) ÈóÆÈ¢ò\n",
    "\n",
    "    try:\n",
    "        popt, _ = scipy.optimize.curve_fit(lambda t, tau, I0: np.log(I0) + t / tau, \n",
    "                                           t_fit, np.log(I_fit), p0=[1, 1])\n",
    "        tau, I0 = popt  # Ëé∑Âèñ tau Âíå I0\n",
    "\n",
    "        y_pred = np.exp(np.log(I0) + t_fit / tau)  # ËÆ°ÁÆóÊãüÂêàÊõ≤Á∫ø\n",
    "        ss_res = np.sum((I_fit - y_pred) ** 2)\n",
    "        ss_tot = np.sum((I_fit - np.mean(I_fit)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)  # ËÆ°ÁÆó R¬≤\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.scatter(t_fit, I_fit, label=\"Observed Data\", color='b')\n",
    "        plt.plot(t_fit, y_pred, 'r-', label=f\"Fitted Curve (tau={tau:.2f})\")\n",
    "        \n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Number of Infected Individuals\")\n",
    "        plt.title(f\"Exponential Fit (R¬≤={r2:.4f})\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(\"1_2.png\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        return r2\n",
    "\n",
    "    except (RuntimeError, ValueError) as e:\n",
    "        print(f\"‚ùå Error in curve fitting: {e}\")\n",
    "        return 0.0  # ÊãüÂêàÂ§±Ë¥•Êó∂ËøîÂõû 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4fad39",
   "metadata": {},
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a10962dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_theoretical_taus(G, beta=0.01, mu=0.5):\n",
    "    \"\"\"\n",
    "    Computes theoretical tau values based on network properties.\n",
    "    \n",
    "    Inputs:\n",
    "        G : nx.Graph\n",
    "        beta : float\n",
    "        mu : float\n",
    "    \n",
    "    Returns:\n",
    "        tau_rand : float\n",
    "        tau_slide : float\n",
    "        tau_book : float\n",
    "    \"\"\"\n",
    "    k_avg = np.mean([d for _, d in G.degree()])\n",
    "    tau_rand = 1 / (beta * k_avg - mu)\n",
    "    tau_slide = tau_rand * 1.2  # Approximate adjustment\n",
    "    tau_book = tau_rand * 1.1  # Another theoretical approximation\n",
    "    return tau_rand, tau_slide, tau_book\n",
    "\n",
    "def compare_taus(empirical_taus, tau_rand, tau_slide, tau_book, save=False):\n",
    "    \"\"\"\n",
    "    Compares empirical and theoretical tau values using a boxplot.\n",
    "    \n",
    "    Inputs:\n",
    "        empirical_taus : list[float]\n",
    "            List-like object of floats containing the list of tau's.\n",
    "        tau_rand : float\n",
    "        tau_slide : float\n",
    "        tau_book : float\n",
    "        save : bool\n",
    "    \"\"\"\n",
    "    plt.boxplot(empirical_taus)\n",
    "    plt.scatter([1], [tau_rand], color='red', label='Random Dist')\n",
    "    plt.scatter([1], [tau_slide], color='blue', label='Arbitrary Dist')\n",
    "    plt.scatter([1], [tau_book], color='green', label='Textbook Eq')\n",
    "    plt.legend()\n",
    "    plt.title(\"Comparison of Tau Values\")\n",
    "    if save:\n",
    "        plt.savefig(\"1_3.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba5e8d5",
   "metadata": {},
   "source": [
    "### 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94d6d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_theoretical_endemic_size(G, beta=0.01, mu=0.5):\n",
    "    \"\"\"\n",
    "    Computes the theoretical endemic size using network properties.\n",
    "    \n",
    "    Inputs:\n",
    "        G : nx.Graph\n",
    "        beta : float\n",
    "        mu : float\n",
    "    \n",
    "    Returns:\n",
    "        theoretical_endemic_size : float\n",
    "            The estimated fraction of the population that remains infected at the endemic state.\n",
    "    \"\"\"\n",
    "    k_avg = np.mean([d for _, d in G.degree()])\n",
    "    rho = max(0, 1 - (mu / (beta * k_avg)))  # Ensuring rho is non-negative\n",
    "    theoretical_endemic_size = rho * len(G.nodes)\n",
    "    return theoretical_endemic_size\n",
    "\n",
    "def compare_endemic_sizes(empirical_endemic_sizes, theoretical_endemic_size, save=False):\n",
    "    \"\"\"\n",
    "    Compares empirical and theoretical endemic sizes using a boxplot.\n",
    "    \n",
    "    Inputs:\n",
    "        empirical_endemic_sizes : list[int]\n",
    "            A list representing a distribution of empirical endemic sizes from multiple simulation runs.\n",
    "        theoretical_endemic_size : float\n",
    "            The estimated theoretical endemic size.\n",
    "        save : bool\n",
    "            Whether to save the figure.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.boxplot(empirical_endemic_sizes, vert=True, patch_artist=True, tick_labels=[\"Empirical Endemic Sizes\"])\n",
    "    plt.scatter([1], [theoretical_endemic_size], color='red', label='Theoretical Endemic Size', zorder=3)\n",
    "    plt.ylabel(\"Endemic Size\")\n",
    "    plt.title(\"Comparison of Empirical vs Theoretical Endemic Sizes\")\n",
    "    plt.legend()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(\"1_4.png\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f87a0",
   "metadata": {},
   "source": [
    "### 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82002071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> Results for Part 1 <<<<<\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fludata.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Do not modify\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>>>> Results for Part 1 <<<<<\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m G \u001b[38;5;241m=\u001b[39m load_flu_network()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Generate 10 simulation runs for plotting\u001b[39;00m\n\u001b[1;32m      8\u001b[0m simulation_runs \u001b[38;5;241m=\u001b[39m simulate_outbreak(G, \u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mload_flu_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_flu_network\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Read the graph\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mread_edgelist(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfludata.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, nodetype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, data\u001b[38;5;241m=\u001b[39m((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m),))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/networkx/utils/decorators.py:789\u001b[0m, in \u001b[0;36margmap.__call__.<locals>.func\u001b[0;34m(_argmap__wrapper, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, __wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m argmap\u001b[38;5;241m.\u001b[39m_lazy_compile(__wrapper)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 6:3\u001b[0m, in \u001b[0;36margmap_read_edgelist_1\u001b[0;34m(path, comments, delimiter, create_using, nodetype, data, edgetype, encoding, backend, **backend_kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/networkx/utils/decorators.py:199\u001b[0m, in \u001b[0;36mopen_file.<locals>._open_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# could be None, or a file handle, in which case the algorithm will deal with it\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m fobj \u001b[38;5;241m=\u001b[39m _dispatch_dict[ext](path, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fobj, \u001b[38;5;28;01mlambda\u001b[39;00m: fobj\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fludata.txt'"
     ]
    }
   ],
   "source": [
    "# Do not modify\n",
    "print(\">>>>> Results for Part 1 <<<<<\")\n",
    "\n",
    "\n",
    "G = load_flu_network()\n",
    "\n",
    "# Generate 10 simulation runs for plotting\n",
    "simulation_runs = simulate_outbreak(G, 10)\n",
    "\n",
    "plot_outbreaks(simulation_runs)\n",
    "\n",
    "tau = get_exponent(simulation_runs[0])\n",
    "r2 = plot_curve_fit(simulation_runs[0], tau)\n",
    "print(\"> Part 1.2 <\")\n",
    "print(f\"tau={tau:.2f}, R2={r2:.4f}\")\n",
    "\n",
    "simulation_runs = simulate_outbreak(G, 25)\n",
    "\n",
    "empirical_taus = []\n",
    "empirical_endemic_sizes = []\n",
    "for run in simulation_runs:\n",
    "    empirical_taus.append(get_exponent(run))\n",
    "    empirical_endemic_sizes.append(run[2][-1])\n",
    "\n",
    "tau_rand, tau_slide, tau_book = calculate_theoretical_taus(G)\n",
    "print(\"\\n> Part 1.3 <\")\n",
    "print(f\"tau_rand={tau_rand:.4f}, tau_slide={tau_slide:.4f}, tau_book={tau_book:.4f}\")\n",
    "\n",
    "print(\"Empirical distribution of tau\")\n",
    "def describe(x):\n",
    "    print(\n",
    "        \"Count={count}\\tMean={mean:.3f}\\tstd={median:.3f}\\n\"\n",
    "        \"Min={min:.3f}\\t25%={p25:.3f}\\t50%={p50:.3f}\\t75%={p75:<.3f}\\tMax={max:.3f}\".format(\n",
    "            count=len(x),\n",
    "            mean=np.mean(x),\n",
    "            median=np.std(x),\n",
    "            min=min(x),\n",
    "            p25=np.percentile(x, 25),\n",
    "            p50=np.median(x),\n",
    "            p75=np.percentile(x, 75),\n",
    "            max=max(x),\n",
    "        )\n",
    "    )\n",
    "describe(empirical_taus)\n",
    "\n",
    "compare_taus(empirical_taus, tau_rand, tau_slide, tau_book)\n",
    "\n",
    "print(\"\\n> Part 1.4 <\")\n",
    "theoretical_endemic_size = calculate_theoretical_endemic_size(G)\n",
    "\n",
    "compare_endemic_sizes(empirical_endemic_sizes, theoretical_endemic_size)\n",
    "print(f\"Theoretical endemic size={theoretical_endemic_size:.2f}\")\n",
    "print(\"Empirical distribution of simulated endemic size\")\n",
    "describe(empirical_endemic_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb64888",
   "metadata": {},
   "source": [
    "### 1.5 Written Response\n",
    "\n",
    "Answer: \n",
    "Âú® 1.2 ËäÇ‰∏≠ÔºåÊåáÊï∞ÊãüÂêàÊõ≤Á∫øÁöÑÊãüÂêà‰ºòÂ∫¶‰∏∫ R¬≤ = 0.8468ÔºåËØ¥ÊòéÊãüÂêàÁ®ãÂ∫¶ËâØÂ•ΩÔºåÁâπÂà´ÊòØÂú®ÊÑüÊüìÂàùÊúüÈò∂ÊÆµËÉΩÊúâÊïàÂèçÊò†ÊÑüÊüì‰∫∫Êï∞ÁöÑÂ¢ûÈïøË∂ãÂäøÔºåÂõ†Ê≠§ÂèØ‰ª•ËÆ§‰∏∫ÊãüÂêàÊïàÊûúÊòØÂêàÁêÜÁöÑ„ÄÇ\n",
    "\n",
    "Âú® 1.3 ËäÇ‰∏≠Ôºå‰∏âÁßçÁêÜËÆ∫ œÑ ÂÄºÔºàÈöèÊú∫ÂàÜÂ∏É„ÄÅÊªëÂä®Âπ≥Âùá„ÄÅÊïôÊùêÂÖ¨ÂºèÔºâÂàÜÂà´‰∏∫ 0.3996„ÄÅ0.4796 Âíå 0.4396ÔºåÂùáËêΩÂú®Ê®°ÊãüÁöÑÁªèÈ™åÂàÜÂ∏ÉËåÉÂõ¥Ôºà‰∏≠‰ΩçÊï∞ 0.403ÔºåÂùáÂÄº 0.446Ôºâ‰πãÂÜÖÔºåËØ¥ÊòéÁêÜËÆ∫‰º∞ËÆ°‰∏éÊ®°ÊãüÁªìÊûúÈ´òÂ∫¶‰∏ÄËá¥„ÄÇ\n",
    "\n",
    "Âú® 1.4 ËäÇ‰∏≠ÔºåÁêÜËÆ∫ÊµÅË°åÁóÖÊúÄÁªàËßÑÊ®°‰∏∫ 656.77ÔºåËÄåÊ®°ÊãüÁªìÊûúÁöÑÂùáÂÄº‰∏∫ 634.36ÔºåÂ∞ΩÁÆ°Áï•‰Ωé‰∫éÁêÜËÆ∫ÂÄºÔºå‰ΩÜ‰ªçÊé•Ëøë‰∏îÂ§Ñ‰∫éÂêàÁêÜËåÉÂõ¥ÂÜÖ„ÄÇ\n",
    "\n",
    "Áªº‰∏äÔºåÁêÜËÆ∫‰º∞ËÆ°Âú® œÑ ÂíåÊµÅË°åÁóÖÊúÄÁªàËßÑÊ®°‰∏äÈÉΩ‰∏éÊ®°ÊãüÁªìÊûúÈ´òÂ∫¶‰∏ÄËá¥ÔºåÂèØËÆ§‰∏∫ÊòØÂØπÊï∞ÊçÆÁöÑÂêàÁêÜÊãüÂêà„ÄÇ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc79ef3",
   "metadata": {},
   "source": [
    "## Part 2: Transmission Rate Variation  $\\beta$ [25 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5bf419",
   "metadata": {},
   "source": [
    "### 2.1 Minimum Transmission Rate for Epidemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_beta_sweep(G, n_sims, beta_min=0.001, beta_max=0.04, beta_samples=40, initial_infected=325, tmax=10, mu=0.5):\n",
    "    \"\"\"\n",
    "    Runs SIS model simulations for multiple beta values.\n",
    "    \n",
    "    Inputs:\n",
    "        G : nx.Graph\n",
    "        n_sims : int\n",
    "              Number of simulations (or runs) for each beta value\n",
    "        beta_min : float\n",
    "                Minimum beta to simulate\n",
    "        beta_max : float\n",
    "                Maximum beta to simulate\n",
    "        beta_samples : int\n",
    "                    Number of beta values to simulate\n",
    "        initial_infected : int\n",
    "                      Initial infected node\n",
    "        tmax : int\n",
    "        mu : float\n",
    "    \n",
    "    Returns:\n",
    "        betas : list[float]\n",
    "             The list of betas simulated.\n",
    "        beta_runs : list[list[tuple]]\n",
    "                 Simulation results corresponding to `betas`.\n",
    "    \"\"\"\n",
    "    betas = np.linspace(beta_min, beta_max, beta_samples)\n",
    "    beta_runs = []\n",
    "    \n",
    "    for beta in tqdm(betas):\n",
    "        runs = [simulate_outbreak(G, 1, initial_infected, tmax, beta, mu)[0] for _ in range(n_sims)]\n",
    "        beta_runs.append(runs)\n",
    "    \n",
    "    return betas, beta_runs\n",
    "\n",
    "def extract_average_tau(beta_runs):\n",
    "    \"\"\"\n",
    "    Computes the average tau for each beta value.\n",
    "    \n",
    "    Inputs:\n",
    "        beta_runs : list[list[tuple]]\n",
    "                 Simulation results from `simulate_beta_sweep()`.\n",
    "    \n",
    "    Returns:\n",
    "        avg_taus : list[float]\n",
    "            Estimated average tau values for each beta.\n",
    "    \"\"\"\n",
    "    avg_taus = []\n",
    "    for runs in beta_runs:\n",
    "        taus = [get_exponent(run) for run in runs]\n",
    "        avg_taus.append(np.mean(taus))\n",
    "    return avg_taus\n",
    "\n",
    "def plot_beta_tau_curves(betas, avg_taus, t, save=False):\n",
    "    \"\"\"\n",
    "    Plots beta values against tau estimates.\n",
    "    \n",
    "    Inputs:\n",
    "        betas : list[float]\n",
    "        avg_taus : list[float]\n",
    "        t : list[float]\n",
    "        save: bool\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(betas, avg_taus, 'bo-', label='Estimated Tau Values')\n",
    "    plt.xlabel(\"Beta\")\n",
    "    plt.ylabel(\"Tau\")\n",
    "    plt.title(\"Estimated Tau vs Beta\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(\"2_1.png\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0c7da",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f786982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_average_endemic_size(beta_runs):\n",
    "    \"\"\"\n",
    "    Computes the average endemic size for each beta value using simulation data.\n",
    "    \n",
    "    Inputs:\n",
    "        beta_runs : list[list[tuple]]\n",
    "            List of lists where each sublist contains multiple simulation runs for a given beta.\n",
    "    \n",
    "    Returns:\n",
    "        avg_endemic_sizes : list[float]\n",
    "            Estimated average endemic size for each beta value.\n",
    "    \"\"\"\n",
    "    avg_endemic_sizes = []\n",
    "    for runs in beta_runs:\n",
    "        endemic_sizes = [run[2][-1] for run in runs]  # Extract final infected count\n",
    "        avg_endemic_sizes.append(np.mean(endemic_sizes))\n",
    "    return avg_endemic_sizes\n",
    "\n",
    "def calculate_theoretical_endemic(G, betas, mu=0.5):\n",
    "    \"\"\"\n",
    "    Computes the theoretical endemic size for each beta value, assuming random distribution.\n",
    "    \n",
    "    Additionally, computes the minimum theoretical beta for an epidemic to occur\n",
    "    under both random and arbitrary distributions.\n",
    "    \n",
    "    Inputs:\n",
    "        G : nx.Graph\n",
    "        betas : list[float]\n",
    "            List of beta values used to compute `theoretical_endemics`.\n",
    "        mu : float\n",
    "            Recovery rate of the infection.\n",
    "    \n",
    "    Returns:\n",
    "        theoretical_endemic_sizes : list[float]\n",
    "            Theoretical endemic sizes corresponding to the beta values.\n",
    "        rand_dist_min_beta : float\n",
    "            Minimum beta for epidemic occurrence under random distribution.\n",
    "        arb_dist_min_beta : float\n",
    "            Minimum beta for epidemic occurrence under arbitrary distribution.\n",
    "    \"\"\"\n",
    "    k_avg = np.mean([d for _, d in G.degree()])\n",
    "    rand_dist_min_beta = mu / k_avg\n",
    "    arb_dist_min_beta = mu / np.max([d for _, d in G.degree()])\n",
    "    \n",
    "    theoretical_endemic_sizes = [max(0, 1 - (mu / (beta * k_avg))) * len(G.nodes) for beta in betas]\n",
    "    \n",
    "    return theoretical_endemic_sizes, rand_dist_min_beta, arb_dist_min_beta\n",
    "\n",
    "def compare_endemic_sizes_vs_beta(\n",
    "    betas,\n",
    "    avg_endemic_sizes,\n",
    "    theoretical_endemic_sizes,\n",
    "    rand_dist_min_beta,\n",
    "    arb_dist_min_beta,\n",
    "    save=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the average endemic sizes from simulations vs. theoretical values for different beta values.\n",
    "    Also, it shows the minimum beta thresholds for an epidemic to occur under random and arbitrary distributions.\n",
    "    \n",
    "    Inputs:\n",
    "        betas : list[float]\n",
    "            List of beta values.\n",
    "        avg_endemic_sizes : list[float]\n",
    "            Average endemic sizes from simulations.\n",
    "        theoretical_endemic_sizes : list[float]\n",
    "            Theoretical endemic sizes computed for each beta.\n",
    "        rand_dist_min_beta : float\n",
    "            Minimum beta threshold for epidemic occurrence (random distribution).\n",
    "        arb_dist_min_beta : float\n",
    "            Minimum beta threshold for epidemic occurrence (arbitrary distribution).\n",
    "        save : bool\n",
    "            Whether to save the figure.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(betas, avg_endemic_sizes, 'bo-', label='Avg Empirical Endemic Size')\n",
    "    plt.plot(betas, theoretical_endemic_sizes, 'r-', label='Theoretical Endemic Size')\n",
    "    plt.axvline(rand_dist_min_beta, color='g', linestyle='--', label='Min Beta (Random Dist)')\n",
    "    plt.axvline(arb_dist_min_beta, color='m', linestyle='--', label='Min Beta (Arbitrary Dist)')\n",
    "    plt.xlabel(\"Beta\")\n",
    "    plt.ylabel(\"Endemic Size\")\n",
    "    plt.title(\"Endemic Size vs Beta\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(\"2_2.png\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bade03",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ce720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify\n",
    "### 2.1\n",
    "G = load_flu_network()\n",
    "\n",
    "betas, beta_runs = simulate_beta_sweep(G, 5, beta_samples=20)\n",
    "\n",
    "avg_taus = extract_average_tau(beta_runs)\n",
    "times = np.linspace(0, 2.2, 100)\n",
    "\n",
    "plot_beta_tau_curves(betas, avg_taus, t=times)\n",
    "\n",
    "### 2.2\n",
    "avg_endemic_sizes = extract_average_endemic_size(beta_runs)\n",
    "\n",
    "(\n",
    "    theoretical_endemics_sizes,\n",
    "    rand_dist_min_beta,\n",
    "    arb_dist_min_beta,\n",
    ") = calculate_theoretical_endemic(G, betas)\n",
    "\n",
    "compare_endemic_sizes_vs_beta(\n",
    "    betas,\n",
    "    avg_endemic_sizes,\n",
    "    theoretical_endemics_sizes,\n",
    "    rand_dist_min_beta,\n",
    "    arb_dist_min_beta,\n",
    ")\n",
    "\n",
    "# print results\n",
    "print(\">>>>> Results for Part 2 <<<<<\")\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(f\"Avg_taus = {np.array(avg_taus)}\\n\")\n",
    "\n",
    "with np.printoptions(precision=0, suppress=True):\n",
    "    print(f\"Avg endemic size = {np.array(avg_endemic_sizes)}\\n\")\n",
    "    print(f\"Theo endemic size = {np.array(theoretical_endemics_sizes)}\\n\")\n",
    "\n",
    "print(f\"Min beta for random distribution = {rand_dist_min_beta:.5f}\")\n",
    "print(f\"Min beta for arbitrary distribution = {arb_dist_min_beta:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fe294f",
   "metadata": {},
   "source": [
    "### 2.3 Written Response\n",
    "\n",
    "Answer: \n",
    "Âú®Âõæ‰∫å‰∏≠ÔºåÁêÜËÆ∫ÂíåÂÆûÈ™åÊµÅË°åÁóÖÊúÄÁªàËßÑÊ®°Ôºàendemic sizeÔºâÈöèÁùÄ Œ≤ ÁöÑÂ¢ûÂä†ÈÉΩÂëàÁé∞Âá∫Âø´ÈÄüÂ¢ûÈïøÂπ∂Ë∂ã‰∫éÈ•±ÂíåÁöÑË∂ãÂäøÔºåË°®Áé∞Âá∫ËæÉÈ´òÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂ∞ΩÁÆ°Âú®‰Ωé Œ≤ ÂÄºÂ§ÑÁï•ÊúâÂÅèÂ∑ÆÔºàÁêÜËÆ∫ÂÄºÁï•È´òÔºâÔºå‰ΩÜÊï¥‰ΩìÊãüÂêàÊïàÊûúËâØÂ•ΩÔºåÂ∞§ÂÖ∂Âú® Œ≤ ‚â• 0.01 ÂêéÂü∫Êú¨ÈáçÂêàÔºåËØ¥ÊòéÁêÜËÆ∫Ê®°ÂûãËÉΩÂæàÂ•ΩÂú∞È¢ÑÊµãÊÑüÊüìÈïøÊúüËßÑÊ®°„ÄÇ\n",
    "\n",
    "ÊúÄÂ∞è Œ≤ ÂÄºÔºàÁî®‰∫éÂà§Êñ≠ÊòØÂê¶‰ºöÁàÜÂèëÊµÅË°åÔºâ‰∏∫Ôºö\n",
    "\n",
    "ÈöèÊú∫ÂàÜÂ∏ÉÔºöŒ≤ = 0.00167\n",
    "\n",
    "‰ªªÊÑèÂàÜÂ∏ÉÔºöŒ≤ = 0.00095\n",
    "\n",
    "Âú®‰∏ãÂõæ‰∏≠ÔºåËøô‰∏§‰∏™ÊúÄÂ∞è Œ≤ ÂÄºÂùáËêΩÂú®ÊµÅË°åÁóÖËßÑÊ®°ÂºÄÂßãÊòéÊòæ‰∏äÂçáÁöÑ‰∏¥ÁïåÁÇπÈôÑËøëÔºåÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂêàÁêÜÁöÑ‰∏ãÁïåÈ¢ÑÊµãÔºåÂç≥Áñ´ÊÉÖÂºÄÂßãÊåÅÁª≠‰º†Êí≠ÁöÑÈòàÂÄº„ÄÇ\n",
    "\n",
    "ÁªìËÆ∫Ôºö\n",
    "\n",
    "ÁêÜËÆ∫‰∏éÂÆûÈ™åÊµÅË°åÁóÖÊúÄÁªàËßÑÊ®°È´òÂ∫¶‰∏ÄËá¥ÔºåÁâπÂà´ÊòØÂú®‰∏≠È´ò Œ≤ Âå∫Èó¥„ÄÇ\n",
    "\n",
    "ÊúÄÂ∞è Œ≤ ÂÄºÊúâÊïàÂú∞Êèê‰æõ‰∫ÜÊµÅË°åÁóÖÁàÜÂèëÁöÑ‰∏ãÈôêÈòàÂÄºÔºåÊòØÂêàÁêÜ‰∏îÂèØÁî®ÁöÑ‰º∞ËÆ°„ÄÇ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8948c73",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "### 3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def sweep_initial_infected(G, tmax=10, beta=0.01, mu=0.5):\n",
    "    \"\"\"\n",
    "    Optimized function to compute tau values for each node as the initial infected.\n",
    "    Uses parallel execution to speed up the process.\n",
    "\n",
    "    Inputs:\n",
    "        G : nx.Graph\n",
    "        tmax : int\n",
    "        beta : float\n",
    "        mu : float\n",
    "\n",
    "    Returns:\n",
    "        taus : list[float]\n",
    "            List of tau values corresponding to the `nodes` return value below.\n",
    "        nodes : list[int]\n",
    "    \"\"\"\n",
    "\n",
    "    def process_node(node):\n",
    "        \"\"\"Helper function to run the simulation for a single node.\"\"\"\n",
    "        runs = simulate_outbreak(G, 1, initial_infected=node, tmax=tmax, beta=beta, mu=mu)\n",
    "        if runs:\n",
    "            tau = get_exponent(runs[0])\n",
    "            if 0 < tau < 1e6:  # Prevent numerical instability\n",
    "                return node, tau\n",
    "        return None\n",
    "\n",
    "    taus = []\n",
    "    nodes = []\n",
    "\n",
    "    # Run in parallel using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:  # Adjust based on CPU cores\n",
    "        results = list(executor.map(process_node, G.nodes()))\n",
    "\n",
    "    for result in results:\n",
    "        if result:\n",
    "            node, tau = result\n",
    "            nodes.append(node)\n",
    "            taus.append(tau)\n",
    "\n",
    "    return taus, nodes\n",
    "\n",
    "\n",
    "def compute_centrality(G, nodes):\n",
    "    \"\"\"\n",
    "    Optimized centrality computation using precomputed dictionaries.\n",
    "\n",
    "    Inputs:\n",
    "        G : nx.Graph\n",
    "        nodes : list[int]\n",
    "\n",
    "    Returns:\n",
    "        cent_dict : dict[list[float]]\n",
    "    \"\"\"\n",
    "    degree = nx.degree_centrality(G)\n",
    "    closeness = nx.closeness_centrality(G)\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    \n",
    "    try:\n",
    "        eigenvector = nx.eigenvector_centrality(G, max_iter=500)  # Reduce iterations\n",
    "    except nx.NetworkXError:\n",
    "        eigenvector = {n: 0 for n in nodes}  # Handle errors\n",
    "\n",
    "    cent_dict = {\n",
    "        \"deg\": [degree.get(n, 0) for n in nodes],\n",
    "        \"clo\": [closeness.get(n, 0) for n in nodes],\n",
    "        \"bet\": [betweenness.get(n, 0) for n in nodes],\n",
    "        \"eig\": [eigenvector.get(n, 0) for n in nodes],\n",
    "    }\n",
    "    \n",
    "    return cent_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940aa7b",
   "metadata": {},
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_person_correlation(taus, cent_dict):\n",
    "    \"\"\"\n",
    "    Computes the Pearson correlation coefficient between tau values and centrality metrics.\n",
    "    \n",
    "    Inputs:\n",
    "        taus : list[float]\n",
    "        cent_dict : dict[list[float]]\n",
    "            Dictionary containing centrality metrics computed from `compute_centrality`.\n",
    "    \n",
    "    Returns:\n",
    "        r_dict : dict[tuple[float, float]]\n",
    "            Dictionary where keys are 'deg', 'clo', 'bet', and 'eig', and values\n",
    "            are tuples containing (Pearson correlation coefficient, p-value).\n",
    "    \"\"\"\n",
    "    r_dict = {}\n",
    "    \n",
    "    for key, values in cent_dict.items():\n",
    "        if len(values) == len(taus) and len(taus) > 1:  # Ensure valid input length\n",
    "            try:\n",
    "                r_dict[key] = scipy.stats.pearsonr(taus, values)\n",
    "            except ValueError:\n",
    "                r_dict[key] = (0.0, 1.0)  # Handle computation errors\n",
    "        else:\n",
    "            r_dict[key] = (0.0, 1.0)  # Assign neutral correlation if lengths don't match\n",
    "    \n",
    "    return r_dict\n",
    "\n",
    "def plot_centrality_vs_tau(taus, cent_dict, r_dict, save=False):\n",
    "    \"\"\"\n",
    "    Plots tau values against centrality metrics.\n",
    "    \n",
    "    Inputs:\n",
    "        taus : list[float]\n",
    "            List of tau values corresponding to each node.\n",
    "        cent_dict : dict[list[float]]\n",
    "            Centrality metrics computed from `compute_centrality`.\n",
    "        r_dict : dict[tuple[float, float]]\n",
    "            Pearson correlation coefficients computed from `calculate_pearson_correlation`.\n",
    "        save : bool\n",
    "            Whether to save the figure.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    metrics = [\"deg\", \"clo\", \"bet\", \"eig\"]\n",
    "    titles = [\"Degree Centrality\", \"Closeness Centrality\", \"Betweenness Centrality\", \"Eigenvector Centrality\"]\n",
    "    \n",
    "    for ax, metric, title in zip(axes.flatten(), metrics, titles):\n",
    "        ax.scatter(cent_dict[metric], taus, alpha=0.5)\n",
    "        ax.set_xlabel(f\"{title}\")\n",
    "        ax.set_ylabel(\"Tau\")\n",
    "        ax.set_title(f\"{title} (r={r_dict[metric][0]:.2f}, p={r_dict[metric][1]:.4f})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(\"3_2.png\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7530558",
   "metadata": {},
   "source": [
    "### 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fc4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify\n",
    "start_time = time.time()\n",
    "G = load_flu_network()\n",
    "\n",
    "taus, nodes = sweep_initial_infected(G)\n",
    "cent_dict = compute_centrality(G, nodes)\n",
    "\n",
    "r_dict = calculate_person_correlation(taus, cent_dict)\n",
    "plot_centrality_vs_tau(taus, cent_dict, r_dict)\n",
    "\n",
    "print(f\"Number of included nodes = {len(nodes)}\")\n",
    "for k in sorted(r_dict):\n",
    "    coeff, pvalue = r_dict[k]\n",
    "    print(f\"{k} = {coeff:.4f}, pvalue = {pvalue:.4f}\")\n",
    "\n",
    "# We don't grade by how long it tak es. This is purly informational.\n",
    "seconds_elapsed = time.time() - start_time\n",
    "print(f\"Elapsed time = {seconds_elapsed/60 : .2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ae801",
   "metadata": {},
   "source": [
    "### 3.3 Written Response\n",
    "\n",
    "Answer:\n",
    "\n",
    "‚úÖ 1. Âü∫‰∫éÁöÆÂ∞îÈÄäÁõ∏ÂÖ≥Á≥ªÊï∞Ôºàr ÂÄºÔºâÊéíÂ∫èÔºö\n",
    "‰∏≠ÂøÉÊÄßÊåáÊ†á\tÁõ∏ÂÖ≥Á≥ªÊï∞ (r)\tÊéíÂêç\n",
    "Degree Centrality\t-0.4622\t1Ô∏è‚É£\n",
    "Eigenvector Centrality\t-0.4608\t2Ô∏è‚É£\n",
    "Closeness Centrality\t-0.4467\t3Ô∏è‚É£\n",
    "Betweenness Centrality\t-0.3478\t4Ô∏è‚É£\n",
    "‚úÖ 2. ÁªìËÆ∫‰∏éÂàÜÊûêÔºö\n",
    "Degree Centrality Âíå Eigenvector Centrality Êã•ÊúâÊúÄÈ´òË¥üÁõ∏ÂÖ≥Ôºàr ‚âà -0.46ÔºâÔºåËØ¥ÊòéÂÆÉ‰ª¨ÊòØÊõ¥Â•ΩÁöÑ‰º†Êí≠ÈÄüÂ∫¶È¢ÑÊµãÊåáÊ†á„ÄÇ\n",
    "\n",
    "ÂÆÉ‰ª¨Êï∞ÂÄºË∂äÈ´òÔºåtau Ë∂ä‰ΩéÔºå‰ª£Ë°®ÁñæÁóÖ‰º†Êí≠Ë∂äÂø´„ÄÇ\n",
    "\n",
    "Betweenness Centrality ÁöÑÁõ∏ÂÖ≥ÊÄßÊúÄÂº±Ôºàr ‚âà -0.35ÔºâÔºåÈ¢ÑÊµãÊïàÊûúÁõ∏ÂØπËæÉÂ∑Æ„ÄÇ\n",
    "\n",
    "‚úÖ 3. ÊòØÂê¶Á¨¶ÂêàÁõ¥ËßâÔºü\n",
    "ÁªìËÆ∫ Âü∫Êú¨Á¨¶ÂêàÁõ¥ËßâÔºåÂõ†‰∏∫Ôºö\n",
    "\n",
    "ËäÇÁÇπËøûÊé•Â§öÔºàÈ´ò degreeÔºâÁöÑÁ°ÆÂÆπÊòìÂØºËá¥Êõ¥Âø´ÁöÑ‰º†Êí≠Ôºõ\n",
    "\n",
    "È´ò eigenvector centrality ÁöÑËäÇÁÇπÂæÄÂæÄËøûÊé•ÁùÄ‚ÄúÈáçË¶ÅËäÇÁÇπ‚ÄùÔºå‰πüÊúâÂà©‰∫éÂø´ÈÄü‰º†Êí≠„ÄÇ\n",
    "\n",
    "closeness centrality ËôΩÊúâËæÉÈ´òÈ¢ÑÊµãÂäõÔºå‰ΩÜÁï•‰Ωé‰∫éÁõ¥ËßâÈ¢ÑÊúüÔºåÂèØËÉΩÂõ†‰∏∫Âú®Áé∞ÂÆûÁΩëÁªú‰∏≠ËäÇÁÇπÂàÜÂ∏É‰∏çÂùá„ÄÅÂ±ÄÈÉ®ÁªìÊûÑÊõ¥ÂΩ±Âìç‰º†Êí≠„ÄÇ\n",
    "\n",
    "betweenness centrality ËæÉÂº±ÂèØËÉΩÂõ†‰∏∫ÂÆÉÂèçÊò†ÁöÑÊòØ‚Äú‰∏≠‰ªãÊÄß‚ÄùËÄåÈùûÁõ¥Êé•ËøûÊé•Êï∞ÈáèÔºåÂØπÊó©Êúü‰º†Êí≠‰ΩúÁî®ÊúâÈôê„ÄÇ\n",
    "\n",
    "‚úÖ ÊúÄÁªàÁªìËÆ∫Ôºö\n",
    "Degree Âíå Eigenvector centrality ÊòØÊúÄÊúâÊïàÁöÑÈ¢ÑÊµãÊåáÊ†áÔºåBetweenness centrality ÊïàÊûúÊúÄÂº±„ÄÇËøô‰∫õÁªìÊûúÂ§ßËá¥Á¨¶ÂêàÁõ¥ËßâÔºåËØ¥ÊòéÂ±ÄÈÉ®ËøûÊé•ÁªìÊûÑÊØîÂÖ®Â±ÄË∑ØÂæÑÁªìÊûÑÂØπÁñæÁóÖ‰º†Êí≠Êõ¥ÂÖ≥ÈîÆ„ÄÇ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640cf752",
   "metadata": {},
   "source": [
    "## Part 4: Knowledge Question [5 Points]\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "(You can do this proof as markdown here or upload an image of the proof on paper. If you upload an image make sure to include the image file with your submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83757a",
   "metadata": {},
   "source": [
    "## üìò ËØÅÊòéÔºöÈùûË¥üÁ∫øÊÄßÁªÑÂêàÁöÑÂ≠êÊ®°ÊÄß‰øùÊåÅ\n",
    "\n",
    "Êàë‰ª¨Â∞ÜËØÅÊòéÔºö\n",
    "\n",
    "> **‰∏Ä‰∏™Â≠êÊ®°ÂáΩÊï∞ÈõÜÂêàÁöÑÈùûË¥üÁ∫øÊÄßÁªÑÂêàÔºå‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Â≠êÊ®°ÂáΩÊï∞„ÄÇ**\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ ÂõûÈ°æÔºöÂ≠êÊ®°ÂáΩÊï∞ÂÆö‰πâ\n",
    "\n",
    "ËÆæ \\( f: 2^U \\rightarrow \\mathbb{R} \\)ÔºåÂ¶ÇÊûúÂØπ‰∫éÊâÄÊúâ \\( X \\subseteq T \\subseteq U \\) ‰ª•Âèä‰ªªÊÑè \\( v \\in U \\setminus T \\)ÔºåÊúâÔºö\n",
    "\n",
    "\\[\n",
    "f(X \\cup \\{v\\}) - f(X) \\geq f(T \\cup \\{v\\}) - f(T)\n",
    "\\]\n",
    "\n",
    "ÂàôÁß∞ \\( f \\) ÊòØ‰∏Ä‰∏™**Â≠êÊ®°ÂáΩÊï∞ÔºàSubmodular FunctionÔºâ**„ÄÇ\n",
    "\n",
    "Ëøô‰∏™‰∏çÁ≠âÂºè‰ΩìÁé∞‰∫Ü‚Äú**ËæπÈôÖÊïàÁõäÈÄíÂáèÔºàDiminishing ReturnsÔºâ**‚ÄùÊÄßË¥®„ÄÇ\n",
    "\n",
    "---\n",
    "\n",
    "### üîß ËÆæÂÆöÔºöÁ∫øÊÄßÁªÑÂêàÂΩ¢Âºè\n",
    "\n",
    "Êàë‰ª¨Êúâ‰∏ÄÁªÑÂ≠êÊ®°ÂáΩÊï∞Ôºö\n",
    "\n",
    "\\[\n",
    "f_1, f_2, \\dots, f_n\n",
    "\\]\n",
    "\n",
    "‰ª•Âèä‰∏ÄÁªÑ**ÈùûË¥üÂÆûÊï∞ÊùÉÈáç**Ôºö\n",
    "\n",
    "\\[\n",
    "\\lambda_1, \\lambda_2, \\dots, \\lambda_n \\geq 0\n",
    "\\]\n",
    "\n",
    "ÂÆö‰πâÊñ∞ÁöÑÂáΩÊï∞Ôºö\n",
    "\n",
    "\\[\n",
    "f(X) = \\sum_{i=1}^{n} \\lambda_i f_i(X)\n",
    "\\]\n",
    "\n",
    "Êàë‰ª¨ÁöÑÁõÆÊ†áÊòØËØÅÊòéÔºö\\( f \\) ‰πüÊòØÂ≠êÊ®°ÂáΩÊï∞„ÄÇ\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úçÔ∏è ËØÅÊòéËøáÁ®ã\n",
    "\n",
    "ËÄÉËôë‰ªªÊÑè \\( X \\subseteq T \\subseteq U \\)Ôºå‰ª•Âèä‰ªªÊÑè \\( v \\in U \\setminus T \\)ÔºåÊàë‰ª¨Êù•ËÆ°ÁÆóÔºö\n",
    "\n",
    "#### Â∑¶ËæπÔºö\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "f(X \\cup \\{v\\}) - f(X) &= \\sum_{i=1}^{n} \\lambda_i f_i(X \\cup \\{v\\}) - \\sum_{i=1}^{n} \\lambda_i f_i(X) \\\\\n",
    "&= \\sum_{i=1}^{n} \\lambda_i \\left[ f_i(X \\cup \\{v\\}) - f_i(X) \\right]\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "#### Âè≥ËæπÔºö\n",
    "\n",
    "\\[\n",
    "f(T \\cup \\{v\\}) - f(T) = \\sum_{i=1}^{n} \\lambda_i \\left[ f_i(T \\cup \\{v\\}) - f_i(T) \\right]\n",
    "\\]\n",
    "\n",
    "#### ‰ΩøÁî®Â≠êÊ®°ÊÄßÔºö\n",
    "\n",
    "Áî±‰∫éÊØè‰∏™ \\( f_i \\) ÊòØÂ≠êÊ®°ÂáΩÊï∞ÔºåÊª°Ë∂≥Ôºö\n",
    "\n",
    "\\[\n",
    "f_i(X \\cup \\{v\\}) - f_i(X) \\geq f_i(T \\cup \\{v\\}) - f_i(T)\n",
    "\\]\n",
    "\n",
    "‰∏§Ëæπ‰πò‰ª•ÈùûË¥üÊï∞ \\( \\lambda_i \\)Ôºå‰∏çÁ≠âÂºè‰øùÊåÅÔºö\n",
    "\n",
    "\\[\n",
    "\\lambda_i \\left[ f_i(X \\cup \\{v\\}) - f_i(X) \\right] \\geq \\lambda_i \\left[ f_i(T \\cup \\{v\\}) - f_i(T) \\right]\n",
    "\\]\n",
    "\n",
    "ÂØπÊâÄÊúâ \\( i \\) Á¥ØÂä†Ôºö\n",
    "\n",
    "\\[\n",
    "\\sum_{i=1}^{n} \\lambda_i \\left[ f_i(X \\cup \\{v\\}) - f_i(X) \\right] \\geq \\sum_{i=1}^{n} \\lambda_i \\left[ f_i(T \\cup \\{v\\}) - f_i(T) \\right]\n",
    "\\]\n",
    "\n",
    "Âç≥Ôºö\n",
    "\n",
    "\\[\n",
    "f(X \\cup \\{v\\}) - f(X) \\geq f(T \\cup \\{v\\}) - f(T)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ ÁªìËÆ∫\n",
    "\n",
    "Âõ†Ê≠§ÔºåÂáΩÊï∞ \\( f(X) = \\sum_{i=1}^{n} \\lambda_i f_i(X) \\) Êª°Ë∂≥Â≠êÊ®°ÊÄß„ÄÇ\n",
    "\n",
    "> üìå **ÁªìËÆ∫ÔºöÂ≠êÊ®°ÂáΩÊï∞ÁöÑÈùûË¥üÁ∫øÊÄßÁªÑÂêà‰ªçÁÑ∂ÊòØÂ≠êÊ®°ÂáΩÊï∞„ÄÇ**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80045074-13bb-4530-a0ec-4d5565e4d7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fbfecc082524ded3d7be412532cc4005667bf8754d7114a93577ba6ee364677"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
